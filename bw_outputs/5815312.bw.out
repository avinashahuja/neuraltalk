----------------------------------------
Begin Torque Prologue on nid25354
at Tue Nov 29 21:01:59 CST 2016
Job Id:			5815312.bw
Username:		tra245
Group:			TRAIN_bacu
Job name:		neruraltalk_16cores
Requested resources:	neednodes=1:ppn=16:xk,nodes=1:ppn=16:xk,walltime=40:00:00
Queue:			normal
Account:		bacu
End Torque Prologue:  0.058 elapsed
----------------------------------------




parsed parameters:
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
}
parsed parameters:
parsed parameters:
parsed parameters:
parsed parameters:
parsed parameters:
parsed parameters:
parsed parameters:
parsed parameters:
parsed parameters:
parsed parameters:
parsed parameters:
parsed parameters:
parsed parameters:
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
}
}
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
}
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
}
}
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
}
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
}
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
}
}
}
}
}
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
}
parsed parameters:
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
}
parsed parameters:
{
  "grad_clip": 5, 
  "max_sentence_length": 40, 
  "rnn_relu_encoders": 0, 
  "dataset": "flickr8k", 
  "image_encoding_size": 256, 
  "eval_max_images": -1, 
  "drop_prob_decoder": 0.5, 
  "word_encoding_size": 256, 
  "max_epochs": 50, 
  "eval_batch_size": 100, 
  "fappend": "baseline", 
  "generator": "lstm", 
  "min_ppl_or_abort": -1, 
  "tanhC_version": 0, 
  "write_checkpoint_ppl_threshold": -1, 
  "decay_rate": 0.999, 
  "rnn_feed_once": 0, 
  "hidden_size": 256, 
  "momentum": 0.0, 
  "worker_status_output_directory": "status/", 
  "init_model_from": "", 
  "learning_rate": 0.001, 
  "checkpoint_output_directory": "cv/", 
  "do_grad_check": 0, 
  "word_count_threshold": 5, 
  "batch_size": 100, 
  "GPU": 0, 
  "regc": 1e-08, 
  "smooth_eps": 1e-08, 
  "solver": "rmsprop", 
  "eval_period": 1.0, 
  "drop_prob_encoder": 0.5
}
Initializing data provider for dataset flickr8k...
BasicDataProvider: reading data/flickr8k/dataset.json
Initializing data provider for dataset flickr8k...
BasicDataProvider: reading data/flickr8k/dataset.json
Initializing data provider for dataset flickr8k...
Initializing data provider for dataset flickr8k...
Initializing data provider for dataset flickr8k...
Initializing data provider for dataset flickr8k...
BasicDataProvider: reading data/flickr8k/dataset.json
BasicDataProvider: reading data/flickr8k/dataset.json
Initializing data provider for dataset flickr8k...
Initializing data provider for dataset flickr8k...
Initializing data provider for dataset flickr8k...
BasicDataProvider: reading data/flickr8k/dataset.json
Initializing data provider for dataset flickr8k...
Initializing data provider for dataset flickr8k...
BasicDataProvider: reading data/flickr8k/dataset.json
BasicDataProvider: reading data/flickr8k/dataset.json
BasicDataProvider: reading data/flickr8k/dataset.json
Initializing data provider for dataset flickr8k...
Initializing data provider for dataset flickr8k...
Initializing data provider for dataset flickr8k...
BasicDataProvider: reading data/flickr8k/dataset.json
BasicDataProvider: reading data/flickr8k/dataset.json
BasicDataProvider: reading data/flickr8k/dataset.json
BasicDataProvider: reading data/flickr8k/dataset.json
BasicDataProvider: reading data/flickr8k/dataset.json
BasicDataProvider: reading data/flickr8k/dataset.json
Initializing data provider for dataset flickr8k...
BasicDataProvider: reading data/flickr8k/dataset.json
Initializing data provider for dataset flickr8k...
BasicDataProvider: reading data/flickr8k/dataset.json
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
BasicDataProvider: reading data/flickr8k/vgg_feats.mat
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
preprocessing word counts and creating vocab based on word count threshold 5
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.34s
filtered words from 7374 to 2537 in 0.35s
0/15000 batch done in 6.493s. at epoch 0.00. loss cost = 323.893372, 
1/15000 batch done in 0.376s. at epoch 0.00. loss cost = 314.311432, 
2/15000 batch done in 0.372s. at epoch 0.01. loss cost = 294.891632, 
3/15000 batch done in 0.372s. at epoch 0.01. loss cost = 270.145233, 
4/15000 batch done in 0.373s. at epoch 0.01. loss cost = 239.237549, 
5/15000 batch done in 0.373s. at epoch 0.02. loss cost = 202.916733, 
6/15000 batch done in 0.374s. at epoch 0.02. loss cost = 158.396072, 
7/15000 batch done in 0.374s. at epoch 0.02. loss cost = 115.128685, 
8/15000 batch done in 0.374s. at epoch 0.03. loss cost = 96.156059, 
9/15000 batch done in 0.374s. at epoch 0.03. loss cost = 85.651268, 
10/15000 batch done in 0.374s. at epoch 0.03. loss cost = 88.622025, 
11/15000 batch done in 0.469s. at epoch 0.04. loss cost = 90.383102, 
12/15000 batch done in 0.374s. at epoch 0.04. loss cost = 96.049484, 
13/15000 batch done in 0.373s. at epoch 0.04. loss cost = 97.465889, 
14/15000 batch done in 0.374s. at epoch 0.05. loss cost = 101.945137, 
15/15000 batch done in 0.372s. at epoch 0.05. loss cost = 106.898239, 
16/15000 batch done in 0.373s. at epoch 0.05. loss cost = 96.901802, 
17/15000 batch done in 0.373s. at epoch 0.06. loss cost = 90.349747, 
18/15000 batch done in 0.373s. at epoch 0.06. loss cost = 103.785248, 
19/15000 batch done in 0.373s. at epoch 0.06. loss cost = 92.326874, 
20/15000 batch done in 0.373s. at epoch 0.07. loss cost = 94.181389, 
21/15000 batch done in 0.374s. at epoch 0.07. loss cost = 91.418259, 
22/15000 batch done in 0.374s. at epoch 0.07. loss cost = 87.744232, 
23/15000 batch done in 0.469s. at epoch 0.08. loss cost = 83.464684, 
24/15000 batch done in 0.374s. at epoch 0.08. loss cost = 87.067467, 
25/15000 batch done in 0.374s. at epoch 0.08. loss cost = 89.783043, 
26/15000 batch done in 0.374s. at epoch 0.09. loss cost = 88.654976, 
27/15000 batch done in 0.374s. at epoch 0.09. loss cost = 91.857399, 
28/15000 batch done in 0.372s. at epoch 0.09. loss cost = 87.294067, 
29/15000 batch done in 0.372s. at epoch 0.10. loss cost = 86.286842, 
30/15000 batch done in 0.373s. at epoch 0.10. loss cost = 87.651909, 
31/15000 batch done in 0.373s. at epoch 0.10. loss cost = 90.558823, 
32/15000 batch done in 0.373s. at epoch 0.11. loss cost = 86.727058, 
33/15000 batch done in 0.373s. at epoch 0.11. loss cost = 85.820908, 
34/15000 batch done in 0.377s. at epoch 0.11. loss cost = 87.374290, 
35/15000 batch done in 0.469s. at epoch 0.12. loss cost = 85.227982, 
36/15000 batch done in 0.373s. at epoch 0.12. loss cost = 90.054596, 
37/15000 batch done in 0.373s. at epoch 0.12. loss cost = 87.006958, 
38/15000 batch done in 0.374s. at epoch 0.13. loss cost = 90.630333, 
39/15000 batch done in 0.374s. at epoch 0.13. loss cost = 86.113174, 
40/15000 batch done in 0.374s. at epoch 0.13. loss cost = 84.067604, 
41/15000 batch done in 0.372s. at epoch 0.14. loss cost = 84.717384, 
42/15000 batch done in 0.372s. at epoch 0.14. loss cost = 89.681946, 
43/15000 batch done in 0.373s. at epoch 0.14. loss cost = 88.548508, 
44/15000 batch done in 0.372s. at epoch 0.15. loss cost = 88.622795, 
45/15000 batch done in 0.374s. at epoch 0.15. loss cost = 86.609619, 
46/15000 batch done in 0.373s. at epoch 0.15. loss cost = 88.286507, 
47/15000 batch done in 0.468s. at epoch 0.16. loss cost = 80.687553, 
48/15000 batch done in 0.374s. at epoch 0.16. loss cost = 85.106369, 
49/15000 batch done in 0.373s. at epoch 0.16. loss cost = 79.458557, 
50/15000 batch done in 0.374s. at epoch 0.17. loss cost = 81.125137, 
51/15000 batch done in 0.376s. at epoch 0.17. loss cost = 79.874893, 
52/15000 batch done in 0.374s. at epoch 0.17. loss cost = 87.570206, 
53/15000 batch done in 0.374s. at epoch 0.18. loss cost = 78.468384, 
54/15000 batch done in 0.375s. at epoch 0.18. loss cost = 82.594162, 
55/15000 batch done in 0.372s. at epoch 0.18. loss cost = 78.672173, 
56/15000 batch done in 0.372s. at epoch 0.19. loss cost = 85.549011, 
57/15000 batch done in 0.373s. at epoch 0.19. loss cost = 76.393044, 
58/15000 batch done in 0.373s. at epoch 0.19. loss cost = 83.661469, 
59/15000 batch done in 0.468s. at epoch 0.20. loss cost = 79.543297, 
60/15000 batch done in 0.374s. at epoch 0.20. loss cost = 82.858253, 
61/15000 batch done in 0.374s. at epoch 0.20. loss cost = 84.619766, 
62/15000 batch done in 0.374s. at epoch 0.21. loss cost = 90.119598, 
63/15000 batch done in 0.374s. at epoch 0.21. loss cost = 82.774719, 
64/15000 batch done in 0.374s. at epoch 0.21. loss cost = 76.673141, 
65/15000 batch done in 0.374s. at epoch 0.22. loss cost = 83.601578, 
66/15000 batch done in 0.374s. at epoch 0.22. loss cost = 81.820168, 
67/15000 batch done in 0.372s. at epoch 0.22. loss cost = 82.096016, 
68/15000 batch done in 0.372s. at epoch 0.23. loss cost = 83.656013, 
69/15000 batch done in 0.374s. at epoch 0.23. loss cost = 69.136063, 
70/15000 batch done in 0.373s. at epoch 0.23. loss cost = 85.302437, 
71/15000 batch done in 0.468s. at epoch 0.24. loss cost = 79.074722, 
72/15000 batch done in 0.374s. at epoch 0.24. loss cost = 79.477608, 
73/15000 batch done in 0.373s. at epoch 0.24. loss cost = 77.337746, 
74/15000 batch done in 0.374s. at epoch 0.25. loss cost = 75.762566, 
75/15000 batch done in 0.374s. at epoch 0.25. loss cost = 76.656227, 
76/15000 batch done in 0.374s. at epoch 0.25. loss cost = 80.674675, 
77/15000 batch done in 0.374s. at epoch 0.26. loss cost = 82.250984, 
78/15000 batch done in 0.374s. at epoch 0.26. loss cost = 80.178467, 
79/15000 batch done in 0.374s. at epoch 0.26. loss cost = 76.807182, 
80/15000 batch done in 0.372s. at epoch 0.27. loss cost = 77.018105, 
81/15000 batch done in 0.372s. at epoch 0.27. loss cost = 76.738693, 
82/15000 batch done in 0.373s. at epoch 0.27. loss cost = 73.549629, 
83/15000 batch done in 0.467s. at epoch 0.28. loss cost = 74.780357, 
84/15000 batch done in 0.373s. at epoch 0.28. loss cost = 81.081856, 
85/15000 batch done in 0.374s. at epoch 0.28. loss cost = 73.632782, 
86/15000 batch done in 0.373s. at epoch 0.29. loss cost = 73.540581, 
87/15000 batch done in 0.374s. at epoch 0.29. loss cost = 74.548553, 
88/15000 batch done in 0.374s. at epoch 0.29. loss cost = 79.770988, 
89/15000 batch done in 0.374s. at epoch 0.30. loss cost = 74.535194, 
90/15000 batch done in 0.374s. at epoch 0.30. loss cost = 75.931129, 
91/15000 batch done in 0.374s. at epoch 0.30. loss cost = 76.172630, 
92/15000 batch done in 0.374s. at epoch 0.31. loss cost = 72.695885, 
93/15000 batch done in 0.373s. at epoch 0.31. loss cost = 75.793259, 
94/15000 batch done in 0.372s. at epoch 0.31. loss cost = 74.858391, 
95/15000 batch done in 0.467s. at epoch 0.32. loss cost = 73.178505, 
96/15000 batch done in 0.373s. at epoch 0.32. loss cost = 76.454018, 
97/15000 batch done in 0.373s. at epoch 0.32. loss cost = 74.911980, 
98/15000 batch done in 0.374s. at epoch 0.33. loss cost = 79.680069, 
99/15000 batch done in 0.373s. at epoch 0.33. loss cost = 73.283043, 
100/15000 batch done in 0.374s. at epoch 0.33. loss cost = 74.675674, 
101/15000 batch done in 0.374s. at epoch 0.34. loss cost = 76.405197, 
102/15000 batch done in 0.374s. at epoch 0.34. loss cost = 77.928490, 
103/15000 batch done in 0.374s. at epoch 0.34. loss cost = 74.128059, 
104/15000 batch done in 0.374s. at epoch 0.35. loss cost = 76.659164, 
105/15000 batch done in 0.374s. at epoch 0.35. loss cost = 77.909904, 
106/15000 batch done in 0.374s. at epoch 0.35. loss cost = 73.725945, 
107/15000 batch done in 0.467s. at epoch 0.36. loss cost = 71.760704, 
108/15000 batch done in 0.375s. at epoch 0.36. loss cost = 70.710892, 
109/15000 batch done in 0.372s. at epoch 0.36. loss cost = 70.247353, 
110/15000 batch done in 0.372s. at epoch 0.37. loss cost = 75.787994, 
111/15000 batch done in 0.373s. at epoch 0.37. loss cost = 71.960419, 
112/15000 batch done in 0.373s. at epoch 0.37. loss cost = 75.492157, 
113/15000 batch done in 0.373s. at epoch 0.38. loss cost = 74.824524, 
114/15000 batch done in 0.374s. at epoch 0.38. loss cost = 73.243622, 
115/15000 batch done in 0.373s. at epoch 0.38. loss cost = 73.630493, 
116/15000 batch done in 0.376s. at epoch 0.39. loss cost = 73.038643, 
117/15000 batch done in 0.374s. at epoch 0.39. loss cost = 74.018097, 
118/15000 batch done in 0.471s. at epoch 0.39. loss cost = 77.366852, 
119/15000 batch done in 0.375s. at epoch 0.40. loss cost = 73.642189, 
120/15000 batch done in 0.374s. at epoch 0.40. loss cost = 75.555618, 
121/15000 batch done in 0.374s. at epoch 0.40. loss cost = 69.469330, 
122/15000 batch done in 0.373s. at epoch 0.41. loss cost = 75.925110, 
123/15000 batch done in 0.372s. at epoch 0.41. loss cost = 72.487160, 
124/15000 batch done in 0.373s. at epoch 0.41. loss cost = 69.830101, 
125/15000 batch done in 0.373s. at epoch 0.42. loss cost = 74.641792, 
126/15000 batch done in 0.373s. at epoch 0.42. loss cost = 71.178062, 
127/15000 batch done in 0.374s. at epoch 0.42. loss cost = 71.356766, 
128/15000 batch done in 0.376s. at epoch 0.43. loss cost = 69.337059, 
129/15000 batch done in 0.374s. at epoch 0.43. loss cost = 76.411797, 
130/15000 batch done in 0.471s. at epoch 0.43. loss cost = 73.111557, 
131/15000 batch done in 0.374s. at epoch 0.44. loss cost = 72.146156, 
132/15000 batch done in 0.374s. at epoch 0.44. loss cost = 72.684830, 
133/15000 batch done in 0.373s. at epoch 0.44. loss cost = 70.940361, 
134/15000 batch done in 0.375s. at epoch 0.45. loss cost = 77.827950, 
135/15000 batch done in 0.372s. at epoch 0.45. loss cost = 73.937584, 
136/15000 batch done in 0.372s. at epoch 0.45. loss cost = 72.752968, 
137/15000 batch done in 0.373s. at epoch 0.46. loss cost = 73.229431, 
138/15000 batch done in 0.373s. at epoch 0.46. loss cost = 72.888847, 
139/15000 batch done in 0.373s. at epoch 0.46. loss cost = 70.483635, 
140/15000 batch done in 0.374s. at epoch 0.47. loss cost = 75.982498, 
141/15000 batch done in 0.373s. at epoch 0.47. loss cost = 70.874596, 
142/15000 batch done in 0.471s. at epoch 0.47. loss cost = 72.871780, 
143/15000 batch done in 0.374s. at epoch 0.48. loss cost = 74.455833, 
144/15000 batch done in 0.374s. at epoch 0.48. loss cost = 72.330727, 
145/15000 batch done in 0.374s. at epoch 0.48. loss cost = 70.419594, 
146/15000 batch done in 0.374s. at epoch 0.49. loss cost = 76.291809, 
147/15000 batch done in 0.374s. at epoch 0.49. loss cost = 73.349030, 
148/15000 batch done in 0.372s. at epoch 0.49. loss cost = 72.171150, 
149/15000 batch done in 0.372s. at epoch 0.50. loss cost = 70.289978, 
150/15000 batch done in 0.373s. at epoch 0.50. loss cost = 72.698296, 
151/15000 batch done in 0.373s. at epoch 0.50. loss cost = 69.601089, 
152/15000 batch done in 0.373s. at epoch 0.51. loss cost = 67.362617, 
153/15000 batch done in 0.373s. at epoch 0.51. loss cost = 70.435555, 
154/15000 batch done in 0.470s. at epoch 0.51. loss cost = 67.031807, 
155/15000 batch done in 0.374s. at epoch 0.52. loss cost = 72.158813, 
156/15000 batch done in 0.374s. at epoch 0.52. loss cost = 74.231659, 
157/15000 batch done in 0.374s. at epoch 0.52. loss cost = 72.333611, 
158/15000 batch done in 0.374s. at epoch 0.53. loss cost = 70.233215, 
159/15000 batch done in 0.374s. at epoch 0.53. loss cost = 68.792999, 
160/15000 batch done in 0.374s. at epoch 0.53. loss cost = 68.222183, 
161/15000 batch done in 0.372s. at epoch 0.54. loss cost = 71.976875, 
162/15000 batch done in 0.372s. at epoch 0.54. loss cost = 67.235664, 
163/15000 batch done in 0.373s. at epoch 0.54. loss cost = 70.223877, 
164/15000 batch done in 0.373s. at epoch 0.55. loss cost = 74.133324, 
165/15000 batch done in 0.373s. at epoch 0.55. loss cost = 75.691307, 
166/15000 batch done in 0.470s. at epoch 0.55. loss cost = 67.931999, 
167/15000 batch done in 0.373s. at epoch 0.56. loss cost = 70.208298, 
168/15000 batch done in 0.374s. at epoch 0.56. loss cost = 72.202637, 
169/15000 batch done in 0.374s. at epoch 0.56. loss cost = 69.907082, 
170/15000 batch done in 0.374s. at epoch 0.57. loss cost = 68.672478, 
171/15000 batch done in 0.375s. at epoch 0.57. loss cost = 73.552803, 
172/15000 batch done in 0.374s. at epoch 0.57. loss cost = 66.946892, 
173/15000 batch done in 0.374s. at epoch 0.58. loss cost = 65.278923, 
174/15000 batch done in 0.372s. at epoch 0.58. loss cost = 66.808952, 
175/15000 batch done in 0.372s. at epoch 0.58. loss cost = 72.006340, 
176/15000 batch done in 0.373s. at epoch 0.59. loss cost = 69.072350, 
177/15000 batch done in 0.373s. at epoch 0.59. loss cost = 66.551041, 
178/15000 batch done in 0.469s. at epoch 0.59. loss cost = 73.347672, 
179/15000 batch done in 0.374s. at epoch 0.60. loss cost = 73.847725, 
180/15000 batch done in 0.374s. at epoch 0.60. loss cost = 70.485977, 
181/15000 batch done in 0.374s. at epoch 0.60. loss cost = 69.023163, 
182/15000 batch done in 0.376s. at epoch 0.61. loss cost = 66.655357, 
183/15000 batch done in 0.374s. at epoch 0.61. loss cost = 65.176964, 
184/15000 batch done in 0.374s. at epoch 0.61. loss cost = 70.434135, 
185/15000 batch done in 0.374s. at epoch 0.62. loss cost = 70.185738, 
186/15000 batch done in 0.374s. at epoch 0.62. loss cost = 69.167519, 
187/15000 batch done in 0.372s. at epoch 0.62. loss cost = 71.143951, 
188/15000 batch done in 0.372s. at epoch 0.63. loss cost = 66.967255, 
189/15000 batch done in 0.373s. at epoch 0.63. loss cost = 69.593483, 
190/15000 batch done in 0.468s. at epoch 0.63. loss cost = 70.155846, 
191/15000 batch done in 0.373s. at epoch 0.64. loss cost = 65.732941, 
192/15000 batch done in 0.374s. at epoch 0.64. loss cost = 67.575546, 
193/15000 batch done in 0.374s. at epoch 0.64. loss cost = 64.984459, 
194/15000 batch done in 0.374s. at epoch 0.65. loss cost = 72.778008, 
195/15000 batch done in 0.374s. at epoch 0.65. loss cost = 67.583534, 
196/15000 batch done in 0.374s. at epoch 0.65. loss cost = 68.646080, 
197/15000 batch done in 0.374s. at epoch 0.66. loss cost = 64.727448, 
198/15000 batch done in 0.374s. at epoch 0.66. loss cost = 66.332993, 
199/15000 batch done in 0.374s. at epoch 0.66. loss cost = 67.968712, 
200/15000 batch done in 0.372s. at epoch 0.67. loss cost = 66.515259, 
201/15000 batch done in 0.372s. at epoch 0.67. loss cost = 69.370102, 
202/15000 batch done in 0.468s. at epoch 0.67. loss cost = 68.168716, 
203/15000 batch done in 0.373s. at epoch 0.68. loss cost = 65.994194, 
204/15000 batch done in 0.374s. at epoch 0.68. loss cost = 66.172966, 
205/15000 batch done in 0.374s. at epoch 0.68. loss cost = 63.303806, 
206/15000 batch done in 0.374s. at epoch 0.69. loss cost = 70.034943, 
207/15000 batch done in 0.374s. at epoch 0.69. loss cost = 67.503181, 
208/15000 batch done in 0.374s. at epoch 0.69. loss cost = 70.455536, 
209/15000 batch done in 0.374s. at epoch 0.70. loss cost = 65.903984, 
210/15000 batch done in 0.374s. at epoch 0.70. loss cost = 67.517448, 
211/15000 batch done in 0.375s. at epoch 0.70. loss cost = 69.427620, 
212/15000 batch done in 0.375s. at epoch 0.71. loss cost = 69.720184, 
213/15000 batch done in 0.375s. at epoch 0.71. loss cost = 66.730553, 
214/15000 batch done in 0.467s. at epoch 0.71. loss cost = 72.509048, 
215/15000 batch done in 0.374s. at epoch 0.72. loss cost = 65.178459, 
216/15000 batch done in 0.372s. at epoch 0.72. loss cost = 72.858109, 
217/15000 batch done in 0.372s. at epoch 0.72. loss cost = 64.818031, 
218/15000 batch done in 0.373s. at epoch 0.73. loss cost = 69.715065, 
219/15000 batch done in 0.373s. at epoch 0.73. loss cost = 70.235970, 
220/15000 batch done in 0.373s. at epoch 0.73. loss cost = 67.690262, 
221/15000 batch done in 0.373s. at epoch 0.74. loss cost = 68.469391, 
222/15000 batch done in 0.373s. at epoch 0.74. loss cost = 63.897533, 
223/15000 batch done in 0.373s. at epoch 0.74. loss cost = 66.270638, 
224/15000 batch done in 0.374s. at epoch 0.75. loss cost = 68.253326, 
225/15000 batch done in 0.470s. at epoch 0.75. loss cost = 67.980759, 
226/15000 batch done in 0.374s. at epoch 0.75. loss cost = 69.539825, 
227/15000 batch done in 0.374s. at epoch 0.76. loss cost = 68.826698, 
228/15000 batch done in 0.374s. at epoch 0.76. loss cost = 68.918114, 
229/15000 batch done in 0.373s. at epoch 0.76. loss cost = 69.619049, 
230/15000 batch done in 0.372s. at epoch 0.77. loss cost = 67.456993, 
231/15000 batch done in 0.373s. at epoch 0.77. loss cost = 67.223122, 
232/15000 batch done in 0.373s. at epoch 0.77. loss cost = 70.726402, 
233/15000 batch done in 0.373s. at epoch 0.78. loss cost = 66.168289, 
234/15000 batch done in 0.374s. at epoch 0.78. loss cost = 67.930000, 
235/15000 batch done in 0.373s. at epoch 0.78. loss cost = 65.554985, 
236/15000 batch done in 0.373s. at epoch 0.79. loss cost = 67.763206, 
237/15000 batch done in 0.471s. at epoch 0.79. loss cost = 67.739876, 
238/15000 batch done in 0.374s. at epoch 0.79. loss cost = 68.664474, 
239/15000 batch done in 0.374s. at epoch 0.80. loss cost = 66.804955, 
240/15000 batch done in 0.374s. at epoch 0.80. loss cost = 63.972092, 
241/15000 batch done in 0.374s. at epoch 0.80. loss cost = 67.364639, 
242/15000 batch done in 0.372s. at epoch 0.81. loss cost = 69.830246, 
243/15000 batch done in 0.372s. at epoch 0.81. loss cost = 65.899971, 
244/15000 batch done in 0.373s. at epoch 0.81. loss cost = 64.528610, 
245/15000 batch done in 0.373s. at epoch 0.82. loss cost = 67.396759, 
246/15000 batch done in 0.373s. at epoch 0.82. loss cost = 68.340416, 
247/15000 batch done in 0.375s. at epoch 0.82. loss cost = 68.473312, 
248/15000 batch done in 0.374s. at epoch 0.83. loss cost = 67.189087, 
249/15000 batch done in 0.471s. at epoch 0.83. loss cost = 66.195602, 
250/15000 batch done in 0.374s. at epoch 0.83. loss cost = 71.806145, 
251/15000 batch done in 0.373s. at epoch 0.84. loss cost = 66.131477, 
252/15000 batch done in 0.374s. at epoch 0.84. loss cost = 64.737251, 
253/15000 batch done in 0.374s. at epoch 0.84. loss cost = 64.104866, 
254/15000 batch done in 0.374s. at epoch 0.85. loss cost = 66.237961, 
255/15000 batch done in 0.372s. at epoch 0.85. loss cost = 65.639511, 
256/15000 batch done in 0.372s. at epoch 0.85. loss cost = 65.623360, 
257/15000 batch done in 0.372s. at epoch 0.86. loss cost = 67.792709, 
258/15000 batch done in 0.373s. at epoch 0.86. loss cost = 64.124260, 
259/15000 batch done in 0.373s. at epoch 0.86. loss cost = 67.511757, 
260/15000 batch done in 0.373s. at epoch 0.87. loss cost = 66.147812, 
261/15000 batch done in 0.470s. at epoch 0.87. loss cost = 64.802307, 
262/15000 batch done in 0.374s. at epoch 0.87. loss cost = 64.467613, 
263/15000 batch done in 0.374s. at epoch 0.88. loss cost = 64.582039, 
264/15000 batch done in 0.374s. at epoch 0.88. loss cost = 69.418152, 
265/15000 batch done in 0.374s. at epoch 0.88. loss cost = 66.625984, 
266/15000 batch done in 0.375s. at epoch 0.89. loss cost = 63.699677, 
267/15000 batch done in 0.374s. at epoch 0.89. loss cost = 63.317287, 
268/15000 batch done in 0.372s. at epoch 0.89. loss cost = 62.497585, 
269/15000 batch done in 0.372s. at epoch 0.90. loss cost = 62.061440, 
270/15000 batch done in 0.372s. at epoch 0.90. loss cost = 64.476151, 
271/15000 batch done in 0.373s. at epoch 0.90. loss cost = 67.039162, 
272/15000 batch done in 0.373s. at epoch 0.91. loss cost = 65.160690, 
273/15000 batch done in 0.470s. at epoch 0.91. loss cost = 64.467949, 
274/15000 batch done in 0.373s. at epoch 0.91. loss cost = 66.904060, 
275/15000 batch done in 0.373s. at epoch 0.92. loss cost = 66.772903, 
276/15000 batch done in 0.374s. at epoch 0.92. loss cost = 63.579044, 
277/15000 batch done in 0.374s. at epoch 0.92. loss cost = 63.255184, 
278/15000 batch done in 0.374s. at epoch 0.93. loss cost = 65.607498, 
279/15000 batch done in 0.374s. at epoch 0.93. loss cost = 70.047417, 
280/15000 batch done in 0.374s. at epoch 0.93. loss cost = 63.211895, 
281/15000 batch done in 0.372s. at epoch 0.94. loss cost = 68.725922, 
282/15000 batch done in 0.372s. at epoch 0.94. loss cost = 65.947563, 
283/15000 batch done in 0.372s. at epoch 0.94. loss cost = 65.782921, 
284/15000 batch done in 0.372s. at epoch 0.95. loss cost = 63.522598, 
285/15000 batch done in 0.469s. at epoch 0.95. loss cost = 65.875061, 
286/15000 batch done in 0.374s. at epoch 0.95. loss cost = 67.081924, 
287/15000 batch done in 0.374s. at epoch 0.96. loss cost = 68.976715, 
288/15000 batch done in 0.374s. at epoch 0.96. loss cost = 65.914551, 
289/15000 batch done in 0.374s. at epoch 0.96. loss cost = 65.203964, 
290/15000 batch done in 0.374s. at epoch 0.97. loss cost = 62.443260, 
291/15000 batch done in 0.374s. at epoch 0.97. loss cost = 63.338821, 
292/15000 batch done in 0.375s. at epoch 0.97. loss cost = 70.512756, 
293/15000 batch done in 0.374s. at epoch 0.98. loss cost = 67.823051, 
294/15000 batch done in 0.372s. at epoch 0.98. loss cost = 66.550186, 
295/15000 batch done in 0.372s. at epoch 0.98. loss cost = 63.677341, 
296/15000 batch done in 0.372s. at epoch 0.99. loss cost = 66.390602, 
297/15000 batch done in 0.468s. at epoch 0.99. loss cost = 63.670277, 
298/15000 batch done in 0.373s. at epoch 0.99. loss cost = 66.064056, 
299/15000 batch done in 0.374s. at epoch 1.00. loss cost = 65.103973, 
evaluating val performance in batches of 100
Application 54067572 exit codes: 1
Application 54067572 resources: utime ~183s, stime ~76s, Rss ~581868, inblocks ~2370395, outblocks ~4232
